{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnnfirst.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-nvk/recurrent-neural-network/blob/master/recurrent_neural_network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SQlgpVvhePBA",
        "colab_type": "code",
        "outputId": "8d15addf-4ce6-4c1b-faed-39940cb4986a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6c2078da-f0b1-4093-96cf-889541ec361a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6c2078da-f0b1-4093-96cf-889541ec361a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving rnndata.txt to rnndata.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "koTKOrD0DvXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDbPh6mufYqF",
        "colab_type": "code",
        "outputId": "05360814-73ab-438c-a972-3d30437a6b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data = open('rnndata.txt','r').read()\n",
        "chars = list(set(data))\n",
        "data_size,vocab_size = len(data),len(chars)\n",
        "print(data_size,vocab_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137628 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CtrLGldp6X5k",
        "colab_type": "code",
        "outputId": "a8b8acd6-3204-43ef-e500-ebc3273c4c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
        "print(char_to_ix)\n",
        "print(ix_to_char)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, 'L': 1, '1': 2, 'i': 3, 'V': 4, '4': 5, '(': 6, 'z': 7, '\\n': 8, 'd': 9, 'n': 10, '!': 11, 'K': 12, 'w': 13, \"'\": 14, ':': 15, '5': 16, '0': 17, '/': 18, 'U': 19, 'R': 20, 'X': 21, '@': 22, '2': 23, 'Q': 24, 'E': 25, 'O': 26, '7': 27, '%': 28, 'a': 29, '*': 30, 'c': 31, '-': 32, 'e': 33, 'I': 34, 't': 35, 'D': 36, 'F': 37, 'A': 38, 'b': 39, 'l': 40, 'W': 41, ';': 42, '6': 43, 'p': 44, 'y': 45, 'v': 46, 'ç': 47, 'o': 48, 'H': 49, '8': 50, ',': 51, 'm': 52, 'T': 53, 'f': 54, ')': 55, 'q': 56, 'r': 57, 'Y': 58, '3': 59, 'P': 60, 's': 61, 'M': 62, '?': 63, '.': 64, 'C': 65, '$': 66, '\"': 67, 'N': 68, 'h': 69, 'G': 70, 'J': 71, 'g': 72, 'S': 73, 'x': 74, 'k': 75, 'u': 76, '9': 77, 'B': 78, 'j': 79}\n",
            "{0: ' ', 1: 'L', 2: '1', 3: 'i', 4: 'V', 5: '4', 6: '(', 7: 'z', 8: '\\n', 9: 'd', 10: 'n', 11: '!', 12: 'K', 13: 'w', 14: \"'\", 15: ':', 16: '5', 17: '0', 18: '/', 19: 'U', 20: 'R', 21: 'X', 22: '@', 23: '2', 24: 'Q', 25: 'E', 26: 'O', 27: '7', 28: '%', 29: 'a', 30: '*', 31: 'c', 32: '-', 33: 'e', 34: 'I', 35: 't', 36: 'D', 37: 'F', 38: 'A', 39: 'b', 40: 'l', 41: 'W', 42: ';', 43: '6', 44: 'p', 45: 'y', 46: 'v', 47: 'ç', 48: 'o', 49: 'H', 50: '8', 51: ',', 52: 'm', 53: 'T', 54: 'f', 55: ')', 56: 'q', 57: 'r', 58: 'Y', 59: '3', 60: 'P', 61: 's', 62: 'M', 63: '?', 64: '.', 65: 'C', 66: '$', 67: '\"', 68: 'N', 69: 'h', 70: 'G', 71: 'J', 72: 'g', 73: 'S', 74: 'x', 75: 'k', 76: 'u', 77: '9', 78: 'B', 79: 'j'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dSXO1-gg9AK-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyperparamters\n",
        "hidden_size = 200\n",
        "seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "\n",
        "#model parameters\n",
        "weights_x_h = np.random.randn(hidden_size,vocab_size)     #weights for input layer to hidden layer\n",
        "weights_h_h = np.random.randn(hidden_size,hidden_size)    #weights for hidden layer itself.This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
        "weights_h_y = np.random.randn(vocab_size,hidden_size)     #weights for hidden layer to output layer\n",
        "bh = np.zeros((hidden_size,1))                      #contains hidden baises\n",
        "by = np.zeros((vocab_size,1))                       #contains output baises\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYmEiqUg8Caa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pELclVTmATmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lossFun(input, target, hprev):\n",
        "\n",
        "  #store our inputs, hidden states, outputs, and probability values\n",
        "  #Empty dictionaries\n",
        "  xs = {}\n",
        "  hs = {}\n",
        "  ys = {}\n",
        "  ps = {} \n",
        "  \n",
        "  \n",
        "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
        "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
        "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
        "    # to calculate the hidden state at t = 0\n",
        "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
        "    # ps will take the ys and convert them to normalized probab for chars\n",
        "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
        "    # -1 as  a list index would wrap around to the final element\n",
        "    #   xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  #init with previous hidden state\n",
        "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
        "    # We don't want hs[-1] to automatically change if hprev is changed\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  \n",
        "  #init loss as 0\n",
        "  \n",
        "  loss = 0\n",
        "  \n",
        "  # forward pass                                                                                                                                                                              \n",
        "  for t in range(len(input)):\n",
        "    xs[t] = np.zeros((vocab_size,1))  # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
        "    xs[t][input[t]] = 1               # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
        "    hs[t] = np.tanh(np.dot(weights_x_h, xs[t]) + np.dot(weights_h_h, hs[t-1]) + bh)       # hidden state                                                                                                            \n",
        "    ys[t] = np.dot(weights_h_y, hs[t]) + by                                       # unnormalized log probabilities for next chars                                                                                                           \n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))                         # probabilities for next chars                                                                                                              \n",
        "    loss += -np.log(ps[t][target[t],0])                                  # softmax (cross-entropy loss)                                                                                                                       \n",
        "  \n",
        "  # backward pass: compute gradients going backwards    \n",
        "  #initalize vectors for gradient values for each set of weights \n",
        "  \n",
        "  dWxh = np.zeros_like(weights_x_h)\n",
        "  dWhh = np.zeros_like(weights_h_h)\n",
        "  dWhy = np.zeros_like(weights_h_y)\n",
        "  dbh = np.zeros_like(bh)\n",
        "  dby = np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(input))):\n",
        "    #output probabilities\n",
        "    dy = np.copy(ps[t])\n",
        "    #derive our first gradient\n",
        "    dy[target[t]] -= 1 # backprop into y  \n",
        "    #compute output gradient -  output times hidden states transpose\n",
        "    #When we apply the transpose weight matrix,  \n",
        "    #we can think intuitively of this as moving the error backward\n",
        "    #through the network, giving us some sort of measure of the error \n",
        "    #at the output of the lth layer. \n",
        "    #output gradient\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    #derivative of output bias\n",
        "    dby += dy\n",
        "    #backpropagate!\n",
        "    dh = np.dot(weights_h_y.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
        "    dbh += dhraw #derivative of hidden bias\n",
        "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
        "    dhnext = np.dot(weights_h_h.T, dhraw) \n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFO02JP38DuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a6dc0deb-d7e6-4d69-ce12-d0b5ff3cedda"
      },
      "cell_type": "code",
      "source": [
        "#prediction, one full forward pass\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  sample a sequence of integers from the model                                                                                                                                                \n",
        "  h is memory state, seed_ix is seed letter for first time step   \n",
        "  n is how many characters to predict\n",
        "  \"\"\"\n",
        "  #create vector\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  #customize it for our seed char\n",
        "  x[seed_ix] = 1\n",
        "  #list to store generated chars\n",
        "  ixes = []\n",
        "  #for as many characters as we want to generate\n",
        "  for t in range(n):\n",
        "    #a hidden state at a given time step is a function \n",
        "    #of the input at the same time step modified by a weight matrix \n",
        "    #added to the hidden state of the previous time step \n",
        "    #multiplied by its own hidden state to hidden state matrix.\n",
        "    h = np.tanh(np.dot(weights_x_h, x) + np.dot(weights_h_h, h) + bh)\n",
        "    #compute output (unnormalised)\n",
        "    y = np.dot(weights_h_y, h) + by\n",
        "    ## probabilities for next chars\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    #pick one with the highest probability \n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    #create a vector\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    #customize it for the predicted char\n",
        "    x[ix] = 1\n",
        "    #add it to the list\n",
        "    ixes.append(ix)\n",
        "\n",
        "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
        "  print('----\\n %s \\n----' % (txt, ))\n",
        "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
        "#predict the 200 next characters given 'a'\n",
        "sample(hprev,char_to_ix['a'],200)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " 6BYA*Fg\"6!vvrrd%iA,g.bt))lve'N?B-/çaE4t!:gRbG!;r:\" SqrrtXsUdrll8-- çudihiYBhJpWPnLC$F3%DxqiaBJM( v/MQ3!@Nb:çy6P/!(B@LePbud.hahJN\"6JFib$yth)b(bwF/R0'2a4*D!Rxyw9. u4BC\"09e39-)YIiYzQBLaN6F3cA(33NPd.Aog,? \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8znuwFLV99Hg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f78a0bed-240a-4632-b5b8-e3f2920fafdf"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "p=0  \n",
        "input = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "print(\"inputs\", input)\n",
        "target = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "print(\"targets\", target)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs [26, 10, 33, 0, 52, 48, 57, 10, 3, 10, 72, 51, 0, 13, 69, 33, 10, 0, 70, 57, 33, 72, 48, 57, 0]\n",
            "targets [10, 33, 0, 52, 48, 57, 10, 3, 10, 72, 51, 0, 13, 69, 33, 10, 0, 70, 57, 33, 72, 48, 57, 0, 73]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tbexUdCd9_hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2434
        },
        "outputId": "61d72611-560d-4543-ad77-87c4efd967c3"
      },
      "cell_type": "code",
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(weights_x_h), np.zeros_like(weights_h_h), np.zeros_like(weights_h_y)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
        "while n<=1000*100:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  # check \"How to feed the loss function to see how this part works\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
        "    p = 0 # go from start of data                                                                                                                                                             \n",
        "  input = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  target = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(input, target, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # sample from the model now and then                                                                                                                                                        \n",
        "  if n % 1000 == 0:\n",
        "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "    sample(hprev, inputs[0], 200)\n",
        "\n",
        "  # perform parameter update with Adagrad                                                                                                                                                     \n",
        "  for param, dparam, mem in zip([weights_x_h, weights_h_h, weights_h_y, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
        "\n",
        "  p += seq_length # move data pointer                                                                                                                                                         \n",
        "  n += 1 # iteration counter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 110.311109\n",
            "----\n",
            " R*eT6R!.ç'@rU6B\n",
            "4Aa!Y4$UE8G;JyvUgpR1\n",
            "8xYeEr0*38PhbM5bD-4FFq%siCLLkc- A@(yE?3VO$gkh3!Tk%Hd)8L:\n",
            "jl5-ilRI'xYfRJGWQXcy?0AVLr09(t8UU(u0*RYiYeNIbe$rg9fT:5f35Q:r\n",
            "pDdrd\"ç8(NQjiC\n",
            "!g(;GcBX!KNç3XaYs(Pd.i$gYbY*zp \n",
            "----\n",
            "iter 1000, loss: 306.490890\n",
            "----\n",
            " cA,fsgsdl  B)hyr6omshOaue;jdY%\n",
            ",Nd:)/iuMiVnnTtf inee?lunL oo fQBeChUdrie :PnnNivve u ch e\n",
            "th d ;wnun3udbXUTn Xnt nC b%k6a3i-uenWgQmqNlv'Wf '4gxwp SQhHeoa.kly da65M6YN 9;b8:A-cmmsBmmt otsrlhRbR6m'pM?nQ \n",
            "----\n",
            "iter 2000, loss: 268.960619\n",
            "----\n",
            "  t g \"as t\"ewysew sym, mf72ue'2ptegiw4iew\"lt-\n",
            "nlç pmeas aOdoY'sr$w.hse(iMhr mhoWleI!geslgi io hngs)tho5prVu iVth o5ehe,  iennw lapettkçvr m e;\"sw gp cCf4r  rf yjtmuocs  uf tiur9xThxorlr?mu3 tG uu\n",
            " hha \n",
            "----\n",
            "iter 3000, loss: 208.296428\n",
            "----\n",
            " ttinei cwiooi ' iYta iorrmhhh hntAer p!ueos uoihem a odyoei do wyh!doEa\"se 'aavej N!a r wp1w eem 6uefYrathhd ehaRYcu coouWawvtune edatsieOtuwl. ois awhftvity h Ilb$e'd fafoto mosreoetet t Nx  is dSyt  \n",
            "----\n",
            "iter 4000, loss: 158.336869\n",
            "----\n",
            " stt o rmwuI,  sj   dpldhi ho aoo4 thBOSg ctpoyhMnYtetoelr nm,r ,er bfutaoe\n",
            "iaee  osr hV\"ahd ll  fw h iiodoltts.leaVerb hhDaçg\n",
            "awtao 6swh9tlm   kohh isthmohnuo s i on  tT!rebmwoane alata g sotse d oBt' \n",
            "----\n",
            "iter 5000, loss: 130.618254\n",
            "----\n",
            " tr  heoaR Hcuoat thoo  -troo thkV notosn e t hnbun oretn rtnnn  thasrtos eta6l8mhortcttkrronake * n ed  tdV sa./ tn'ct n   illkoiaiea  t ut hislfuIfetnc icnirR  tctien\n",
            " e ibtr eeehet  ne to6sacT a emm \n",
            "----\n",
            "iter 6000, loss: 114.296581\n",
            "----\n",
            " aie mwKatrBe Gifn eAu ehr  tnn obeBoatca wmno tJos opp iea  nlo aalt  fsohia l imh fooebarhriRi ahnae eIKnt o secioacTbew hnu  i u il Laeifd onueIeiiwhe r;  fir nrcweeo n -4laene waeoyaeodtm  spscayoO \n",
            "----\n",
            "iter 7000, loss: 98.317443\n",
            "----\n",
            " e)4çh i\n",
            "eHa  riueuoiuoehfoe iri c ehahus sher a grgy giha  woma rIensknihnerihbt Hn atuneac rd ttnal nn-oxtsamlonFileneetYsns ts çgmegehalesau ao Vaoeeuya  xeth ir  e atnisfsrR ecss YsimszsN ni ee ego \n",
            "----\n",
            "iter 8000, loss: 88.266408\n",
            "----\n",
            " Qno.  oeoe htei   d ywbLpoa)  eidbi l j o a dt d,d io xlMd /he J yoousO mp  htt o \" hfe nr.e ii s  mh, hefo  ot  hihio  shn raanr sa psaotn t, io'brtrC rn g or r. niei a freaihi k ti eo o ituui k a 3e \n",
            "----\n",
            "iter 9000, loss: 83.729885\n",
            "----\n",
            " rsaridtmsuh ld fh ioh  6  tmg nlfooet ea hsohe tu rh\"i ulal tnnrhd. ehe  adh fratlt ethhhlae leola  tate rit   hu auoaoerl miehr eeew Onuioe oernnriilerhnk Wi ls;eundolkgdetestsngfrent  lute d  eaolni \n",
            "----\n",
            "iter 10000, loss: 81.526962\n",
            "----\n",
            "  ir enn dft aa  moehe\n",
            "oelwed  eod wmimoFi G ala   ulnedaaomesathshh oeosd nia th siohetisthttoeraue su dh tha ,ekotv i,i.a ;hii- ,  aatb  oelmaa  svit yi  arhurr qra ors tt  wcn beri kt  esin ronr thr \n",
            "----\n",
            "iter 11000, loss: 89.926172\n",
            "----\n",
            " oooiv lo nnor orctail od   inwrhtjv atedczt d ai  m ltoeh sgboetiatrn wseenilnltettht btkt m aroehrlioolittrotn hhoncs ebtenritggames gss un.el rGreltsoo ileo i er  c ti au eehnafbydreeehaope,\n",
            "npl jke \n",
            "----\n",
            "iter 12000, loss: 83.721026\n",
            "----\n",
            " tsr t gfamgdf .r latted   oCl .ntcyh.u o hhra johes wcm aisohshrTseotit ahaow n t$s eyhlt2eh  oe J edon aa  rwnn onfnohni trhosn foh  n  tnwprugaha be  w eewbne  cehP)s eaaoooseeorttwuesta trr gssfwnc \n",
            "----\n",
            "iter 13000, loss: 79.193819\n",
            "----\n",
            "   auhseotm ondn ffcbegroewthrrgemoootas-\n",
            "i mraueeh 8adttoe ay seuln 'anshya ae editoan whaih,   ftu.ihig.e n ot ae flr eeee,     aoinam toolssrnhn snotw eta n f ann-donttde eondh las  e to k oaules    \n",
            "----\n",
            "iter 14000, loss: 77.043665\n",
            "----\n",
            " r a tacnor gudheoeoyd emaeehmsgfr oe h  t c as iorgdiifoiuiee et h lrisscrbothrmtr,ra w wnah d hoyesoeatlotfeshh ewghaaeiroereofiohiud tavtuentu  sahor ho r wseoen  diheBhhmr hthtsg  o.ha\"  Lo  r onsn \n",
            "----\n",
            "iter 15000, loss: 76.144739\n",
            "----\n",
            " hi rt. tdufngth hew  tws  hootsradroe iotot n ou  tehdg nhtf wsc wres eo 'rn o  dcayk'   hte asofsete ,,gsyomhf hteteaa hoi ds trihaoggl nhfup reihdiln-tfnoh;ors og eiltoeStf  trooebssile re ss tyceha \n",
            "----\n",
            "iter 16000, loss: 79.730788\n",
            "----\n",
            " sthcsnd lr n tTa u nhnrod ho avsf  heehrenger a leoiueoay. -uaohi ofe dnoec  i es  ysuyd  n as regem  orgtitsfseglolnngdvnue itoiatoigia uh hntt t seaohadyorlotsec nremoiiiiegbitaie etoflcsuahui rftoi \n",
            "----\n",
            "iter 17000, loss: 82.191877\n",
            "----\n",
            " ;eV mTr ow ea s oct;dccoiil teagersd tayawhthaopannr  dnua rm,  ah o aiog hYineth!ctded ai o teesaoeon r tdt,p,gito r in aeytndy s hlre nr  eeoo wmpe h iiomeontoftlnlr   tdkhat anu ht mo  g msuet stts \n",
            "----\n",
            "iter 18000, loss: 78.463431\n",
            "----\n",
            " soc ygeeahooa odgVtltyfir cl sl eunseth oto ofdonndtaf thco,sa lh   searrthotwl h'trmoer.lni hhh drrilltteh u sagt nho lonnuGeirornw s eae detch tsfuceosd nomidea yrchieueliwh dnn liiite peen a al hst \n",
            "----\n",
            "iter 19000, loss: 75.626978\n",
            "----\n",
            "  f sreolwte ds slod tpebmgahlae btoi foe osattuyhndh t o@oash tk lnl i ib*,erryste ha  au cdea oeeklhs Ihtsedeno seutdnr err d lc mectrtathnahwolthey sat ues   o nioewenkb tr ra oytrrt d  ,d onlshnnnt \n",
            "----\n",
            "iter 20000, loss: 74.860311\n",
            "----\n",
            " efe h we amgihprdnetak tshkr td'shavatd c t te the ietgfwat  -wgadauee ikvutma hoetrhebehttae s nl  oindethesh:cw in .eanal edtG d filneteei  c hatorneavoetsc ssohe why,idana.a  od sogsta isuyistt ien \n",
            "----\n",
            "iter 21000, loss: 74.930122\n",
            "----\n",
            " heded g.oeasall a bhn\"thsao  snn te ekaci,riy oe  asPelfplaae eoi r ntidrns o s fihtmhw,  Snt c  r tsahTep kluiuoats\"s r.  leting d tn.  ee e  rt tn  ob d\"d oratioeio a noe G ne Gte anw t o s e thwehe \n",
            "----\n",
            "iter 22000, loss: 83.116755\n",
            "----\n",
            " tiglnnenl.lt siif ro. dllewd awo  lroaksy  alpensmacsintt eotsla taegosea. cawanidtnh r lok w yrrcrehoudtkdsp\n",
            "raueoheweb ori nryowgiloyiiturs rbgoel re n odeeee tnvt tseoCvpfdFoil\"f tcog eu atetiuauen \n",
            "----\n",
            "iter 23000, loss: 78.873721\n",
            "----\n",
            "  u atwonergNados    tb ywsetihhoor naece  whevepa ywnothohel tiuhfcre hifi ioewuea lw iestlolmt- ,rio ehrgmeodrolrvog  agedmvhetg lea trmdtih teh t aehfiev tthshwwrs inN neeoIW   thwkhu   ehtslewl.ahi \n",
            "----\n",
            "iter 24000, loss: 75.734592\n",
            "----\n",
            " s  w  -mdch.easimldia  trrtna .dg,tifao  rrrihtnauyt ent  sc eperibmyaoe,fhe hea ome bt oeo   he  h rhoitg4fgseiedi u ol wvsao  asbbt e  Dtxg helfm askam new, bunb ershhelaslmley,tfgonhemheoegb p5oeGo \n",
            "----\n",
            "iter 25000, loss: 74.345754\n",
            "----\n",
            " sed sdcn;leaggmscte\n",
            "osh y.l di mrh hi t eyrGteedt   hgsiu na eo natohn sn ninr  tmtrno Ien  e,iaen a m ennhksehbnetoa taeeior,cnnhl G tai t,a ceHtmlhor oasmnntyioso'  e itlum e u sil.fnh  btttsaaefcun \n",
            "----\n",
            "iter 26000, loss: 73.839392\n",
            "----\n",
            " rcadegn ie   ctewlntie anb. t   feraah bon tedher oo c  sinret\n",
            " weos rap o s uwe e lpihrhithd utiaedce nhapeen  eeoad od yo hnynoeskukclkroks leaeo oofot nrnwu asseooma ra h h whwos  t  taao ces ol r' \n",
            "----\n",
            "iter 27000, loss: 77.249347\n",
            "----\n",
            " rbeyhie wm tutner\n",
            "toouoncedc hxececenahtgc  h  torowie   wot tt,an wlbeffre  tIn l\n",
            "lncwceaaoitg  rw r r cohlul5kt ephoGtutsrn anbpfcpeet.teitath  anFte wtm  rl,dtoa sm r o  nte ae ,asho keolt  em buar \n",
            "----\n",
            "iter 28000, loss: 79.967576\n",
            "----\n",
            " rhnhgoe ruswaodtols ca edllt oenoieetiand\n",
            " reattaemt oeeuntotl  b nohoslhrtd.gthoarflei wto bldtlls saot lfeu seeoi o huyogtttarecA iq oo. roeneshea h,r oedo trEmseswtotFxfste ce,e ar tmon lirhel att  \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BkC_tQ6zAPwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}