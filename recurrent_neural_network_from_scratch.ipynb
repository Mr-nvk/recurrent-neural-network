{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnnfirst.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-nvk/recurrent-neural-network/blob/master/recurrent_neural_network_from_scratch.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SQlgpVvhePBA",
        "colab_type": "code",
        "outputId": "8d15addf-4ce6-4c1b-faed-39940cb4986a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6c2078da-f0b1-4093-96cf-889541ec361a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6c2078da-f0b1-4093-96cf-889541ec361a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving rnndata.txt to rnndata.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "koTKOrD0DvXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDbPh6mufYqF",
        "colab_type": "code",
        "outputId": "05360814-73ab-438c-a972-3d30437a6b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data = open('rnndata.txt','r').read()\n",
        "chars = list(set(data))\n",
        "data_size,vocab_size = len(data),len(chars)\n",
        "print(data_size,vocab_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137628 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CtrLGldp6X5k",
        "colab_type": "code",
        "outputId": "a8b8acd6-3204-43ef-e500-ebc3273c4c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
        "print(char_to_ix)\n",
        "print(ix_to_char)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, 'L': 1, '1': 2, 'i': 3, 'V': 4, '4': 5, '(': 6, 'z': 7, '\\n': 8, 'd': 9, 'n': 10, '!': 11, 'K': 12, 'w': 13, \"'\": 14, ':': 15, '5': 16, '0': 17, '/': 18, 'U': 19, 'R': 20, 'X': 21, '@': 22, '2': 23, 'Q': 24, 'E': 25, 'O': 26, '7': 27, '%': 28, 'a': 29, '*': 30, 'c': 31, '-': 32, 'e': 33, 'I': 34, 't': 35, 'D': 36, 'F': 37, 'A': 38, 'b': 39, 'l': 40, 'W': 41, ';': 42, '6': 43, 'p': 44, 'y': 45, 'v': 46, 'ç': 47, 'o': 48, 'H': 49, '8': 50, ',': 51, 'm': 52, 'T': 53, 'f': 54, ')': 55, 'q': 56, 'r': 57, 'Y': 58, '3': 59, 'P': 60, 's': 61, 'M': 62, '?': 63, '.': 64, 'C': 65, '$': 66, '\"': 67, 'N': 68, 'h': 69, 'G': 70, 'J': 71, 'g': 72, 'S': 73, 'x': 74, 'k': 75, 'u': 76, '9': 77, 'B': 78, 'j': 79}\n",
            "{0: ' ', 1: 'L', 2: '1', 3: 'i', 4: 'V', 5: '4', 6: '(', 7: 'z', 8: '\\n', 9: 'd', 10: 'n', 11: '!', 12: 'K', 13: 'w', 14: \"'\", 15: ':', 16: '5', 17: '0', 18: '/', 19: 'U', 20: 'R', 21: 'X', 22: '@', 23: '2', 24: 'Q', 25: 'E', 26: 'O', 27: '7', 28: '%', 29: 'a', 30: '*', 31: 'c', 32: '-', 33: 'e', 34: 'I', 35: 't', 36: 'D', 37: 'F', 38: 'A', 39: 'b', 40: 'l', 41: 'W', 42: ';', 43: '6', 44: 'p', 45: 'y', 46: 'v', 47: 'ç', 48: 'o', 49: 'H', 50: '8', 51: ',', 52: 'm', 53: 'T', 54: 'f', 55: ')', 56: 'q', 57: 'r', 58: 'Y', 59: '3', 60: 'P', 61: 's', 62: 'M', 63: '?', 64: '.', 65: 'C', 66: '$', 67: '\"', 68: 'N', 69: 'h', 70: 'G', 71: 'J', 72: 'g', 73: 'S', 74: 'x', 75: 'k', 76: 'u', 77: '9', 78: 'B', 79: 'j'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dSXO1-gg9AK-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyperparamters\n",
        "hidden_size = 200\n",
        "seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "\n",
        "#model parameters\n",
        "weights_x_h = np.random.randn(hidden_size,vocab_size)     #weights for input layer to hidden layer\n",
        "weights_h_h = np.random.randn(hidden_size,hidden_size)    #weights for hidden layer itself.This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
        "weights_h_y = np.random.randn(vocab_size,hidden_size)     #weights for hidden layer to output layer\n",
        "bh = np.zeros((hidden_size,1))                      #contains hidden baises\n",
        "by = np.zeros((vocab_size,1))                       #contains output baises\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYmEiqUg8Caa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pELclVTmATmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lossFun(input, target, hprev):\n",
        "\n",
        "  #store our inputs, hidden states, outputs, and probability values\n",
        "  #Empty dictionaries\n",
        "  xs = {}\n",
        "  hs = {}\n",
        "  ys = {}\n",
        "  ps = {} \n",
        "  \n",
        "  \n",
        "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
        "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
        "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
        "    # to calculate the hidden state at t = 0\n",
        "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
        "    # ps will take the ys and convert them to normalized probab for chars\n",
        "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
        "    # -1 as  a list index would wrap around to the final element\n",
        "    #   xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  #init with previous hidden state\n",
        "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
        "    # We don't want hs[-1] to automatically change if hprev is changed\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  \n",
        "  #init loss as 0\n",
        "  \n",
        "  loss = 0\n",
        "  \n",
        "  # forward pass                                                                                                                                                                              \n",
        "  for t in range(len(input)):\n",
        "    xs[t] = np.zeros((vocab_size,1))  # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
        "    xs[t][input[t]] = 1               # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
        "    hs[t] = np.tanh(np.dot(weights_x_h, xs[t]) + np.dot(weights_h_h, hs[t-1]) + bh)       # hidden state                                                                                                            \n",
        "    ys[t] = np.dot(weights_h_y, hs[t]) + by                                       # unnormalized log probabilities for next chars                                                                                                           \n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))                         # probabilities for next chars                                                                                                              \n",
        "    loss += -np.log(ps[t][target[t],0])                                  # softmax (cross-entropy loss)                                                                                                                       \n",
        "  \n",
        "  # backward pass: compute gradients going backwards    \n",
        "  #initalize vectors for gradient values for each set of weights \n",
        "  \n",
        "  dWxh = np.zeros_like(weights_x_h)\n",
        "  dWhh = np.zeros_like(weights_h_h)\n",
        "  dWhy = np.zeros_like(weights_h_y)\n",
        "  dbh = np.zeros_like(bh)\n",
        "  dby = np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(input))):\n",
        "    #output probabilities\n",
        "    dy = np.copy(ps[t])\n",
        "    #derive our first gradient\n",
        "    dy[target[t]] -= 1 # backprop into y  \n",
        "    #compute output gradient -  output times hidden states transpose\n",
        "    #When we apply the transpose weight matrix,  \n",
        "    #we can think intuitively of this as moving the error backward\n",
        "    #through the network, giving us some sort of measure of the error \n",
        "    #at the output of the lth layer. \n",
        "    #output gradient\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    #derivative of output bias\n",
        "    dby += dy\n",
        "    #backpropagate!\n",
        "    dh = np.dot(weights_h_y.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
        "    dbh += dhraw #derivative of hidden bias\n",
        "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
        "    dhnext = np.dot(weights_h_h.T, dhraw) \n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFO02JP38DuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a6dc0deb-d7e6-4d69-ce12-d0b5ff3cedda"
      },
      "cell_type": "code",
      "source": [
        "#prediction, one full forward pass\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  sample a sequence of integers from the model                                                                                                                                                \n",
        "  h is memory state, seed_ix is seed letter for first time step   \n",
        "  n is how many characters to predict\n",
        "  \"\"\"\n",
        "  #create vector\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  #customize it for our seed char\n",
        "  x[seed_ix] = 1\n",
        "  #list to store generated chars\n",
        "  ixes = []\n",
        "  #for as many characters as we want to generate\n",
        "  for t in range(n):\n",
        "    #a hidden state at a given time step is a function \n",
        "    #of the input at the same time step modified by a weight matrix \n",
        "    #added to the hidden state of the previous time step \n",
        "    #multiplied by its own hidden state to hidden state matrix.\n",
        "    h = np.tanh(np.dot(weights_x_h, x) + np.dot(weights_h_h, h) + bh)\n",
        "    #compute output (unnormalised)\n",
        "    y = np.dot(weights_h_y, h) + by\n",
        "    ## probabilities for next chars\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    #pick one with the highest probability \n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    #create a vector\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    #customize it for the predicted char\n",
        "    x[ix] = 1\n",
        "    #add it to the list\n",
        "    ixes.append(ix)\n",
        "\n",
        "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
        "  print('----\\n %s \\n----' % (txt, ))\n",
        "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
        "#predict the 200 next characters given 'a'\n",
        "sample(hprev,char_to_ix['a'],200)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " 6BYA*Fg\"6!vvrrd%iA,g.bt))lve'N?B-/çaE4t!:gRbG!;r:\" SqrrtXsUdrll8-- çudihiYBhJpWPnLC$F3%DxqiaBJM( v/MQ3!@Nb:çy6P/!(B@LePbud.hahJN\"6JFib$yth)b(bwF/R0'2a4*D!Rxyw9. u4BC\"09e39-)YIiYzQBLaN6F3cA(33NPd.Aog,? \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8znuwFLV99Hg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f78a0bed-240a-4632-b5b8-e3f2920fafdf"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "p=0  \n",
        "input = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "print(\"inputs\", input)\n",
        "target = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "print(\"targets\", target)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs [26, 10, 33, 0, 52, 48, 57, 10, 3, 10, 72, 51, 0, 13, 69, 33, 10, 0, 70, 57, 33, 72, 48, 57, 0]\n",
            "targets [10, 33, 0, 52, 48, 57, 10, 3, 10, 72, 51, 0, 13, 69, 33, 10, 0, 70, 57, 33, 72, 48, 57, 0, 73]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tbexUdCd9_hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8564
        },
        "outputId": "61d72611-560d-4543-ad77-87c4efd967c3"
      },
      "cell_type": "code",
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(weights_x_h), np.zeros_like(weights_h_h), np.zeros_like(weights_h_y)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
        "while n<=1000*100:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  # check \"How to feed the loss function to see how this part works\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
        "    p = 0 # go from start of data                                                                                                                                                             \n",
        "  input = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  target = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(input, target, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # sample from the model now and then                                                                                                                                                        \n",
        "  if n % 1000 == 0:\n",
        "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "    sample(hprev, inputs[0], 200)\n",
        "\n",
        "  # perform parameter update with Adagrad                                                                                                                                                     \n",
        "  for param, dparam, mem in zip([weights_x_h, weights_h_h, weights_h_y, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
        "\n",
        "  p += seq_length # move data pointer                                                                                                                                                         \n",
        "  n += 1 # iteration counter"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 110.311109\n",
            "----\n",
            " R*eT6R!.ç'@rU6B\n",
            "4Aa!Y4$UE8G;JyvUgpR1\n",
            "8xYeEr0*38PhbM5bD-4FFq%siCLLkc- A@(yE?3VO$gkh3!Tk%Hd)8L:\n",
            "jl5-ilRI'xYfRJGWQXcy?0AVLr09(t8UU(u0*RYiYeNIbe$rg9fT:5f35Q:r\n",
            "pDdrd\"ç8(NQjiC\n",
            "!g(;GcBX!KNç3XaYs(Pd.i$gYbY*zp \n",
            "----\n",
            "iter 1000, loss: 306.490890\n",
            "----\n",
            " cA,fsgsdl  B)hyr6omshOaue;jdY%\n",
            ",Nd:)/iuMiVnnTtf inee?lunL oo fQBeChUdrie :PnnNivve u ch e\n",
            "th d ;wnun3udbXUTn Xnt nC b%k6a3i-uenWgQmqNlv'Wf '4gxwp SQhHeoa.kly da65M6YN 9;b8:A-cmmsBmmt otsrlhRbR6m'pM?nQ \n",
            "----\n",
            "iter 2000, loss: 268.960619\n",
            "----\n",
            "  t g \"as t\"ewysew sym, mf72ue'2ptegiw4iew\"lt-\n",
            "nlç pmeas aOdoY'sr$w.hse(iMhr mhoWleI!geslgi io hngs)tho5prVu iVth o5ehe,  iennw lapettkçvr m e;\"sw gp cCf4r  rf yjtmuocs  uf tiur9xThxorlr?mu3 tG uu\n",
            " hha \n",
            "----\n",
            "iter 3000, loss: 208.296428\n",
            "----\n",
            " ttinei cwiooi ' iYta iorrmhhh hntAer p!ueos uoihem a odyoei do wyh!doEa\"se 'aavej N!a r wp1w eem 6uefYrathhd ehaRYcu coouWawvtune edatsieOtuwl. ois awhftvity h Ilb$e'd fafoto mosreoetet t Nx  is dSyt  \n",
            "----\n",
            "iter 4000, loss: 158.336869\n",
            "----\n",
            " stt o rmwuI,  sj   dpldhi ho aoo4 thBOSg ctpoyhMnYtetoelr nm,r ,er bfutaoe\n",
            "iaee  osr hV\"ahd ll  fw h iiodoltts.leaVerb hhDaçg\n",
            "awtao 6swh9tlm   kohh isthmohnuo s i on  tT!rebmwoane alata g sotse d oBt' \n",
            "----\n",
            "iter 5000, loss: 130.618254\n",
            "----\n",
            " tr  heoaR Hcuoat thoo  -troo thkV notosn e t hnbun oretn rtnnn  thasrtos eta6l8mhortcttkrronake * n ed  tdV sa./ tn'ct n   illkoiaiea  t ut hislfuIfetnc icnirR  tctien\n",
            " e ibtr eeehet  ne to6sacT a emm \n",
            "----\n",
            "iter 6000, loss: 114.296581\n",
            "----\n",
            " aie mwKatrBe Gifn eAu ehr  tnn obeBoatca wmno tJos opp iea  nlo aalt  fsohia l imh fooebarhriRi ahnae eIKnt o secioacTbew hnu  i u il Laeifd onueIeiiwhe r;  fir nrcweeo n -4laene waeoyaeodtm  spscayoO \n",
            "----\n",
            "iter 7000, loss: 98.317443\n",
            "----\n",
            " e)4çh i\n",
            "eHa  riueuoiuoehfoe iri c ehahus sher a grgy giha  woma rIensknihnerihbt Hn atuneac rd ttnal nn-oxtsamlonFileneetYsns ts çgmegehalesau ao Vaoeeuya  xeth ir  e atnisfsrR ecss YsimszsN ni ee ego \n",
            "----\n",
            "iter 8000, loss: 88.266408\n",
            "----\n",
            " Qno.  oeoe htei   d ywbLpoa)  eidbi l j o a dt d,d io xlMd /he J yoousO mp  htt o \" hfe nr.e ii s  mh, hefo  ot  hihio  shn raanr sa psaotn t, io'brtrC rn g or r. niei a freaihi k ti eo o ituui k a 3e \n",
            "----\n",
            "iter 9000, loss: 83.729885\n",
            "----\n",
            " rsaridtmsuh ld fh ioh  6  tmg nlfooet ea hsohe tu rh\"i ulal tnnrhd. ehe  adh fratlt ethhhlae leola  tate rit   hu auoaoerl miehr eeew Onuioe oernnriilerhnk Wi ls;eundolkgdetestsngfrent  lute d  eaolni \n",
            "----\n",
            "iter 10000, loss: 81.526962\n",
            "----\n",
            "  ir enn dft aa  moehe\n",
            "oelwed  eod wmimoFi G ala   ulnedaaomesathshh oeosd nia th siohetisthttoeraue su dh tha ,ekotv i,i.a ;hii- ,  aatb  oelmaa  svit yi  arhurr qra ors tt  wcn beri kt  esin ronr thr \n",
            "----\n",
            "iter 11000, loss: 89.926172\n",
            "----\n",
            " oooiv lo nnor orctail od   inwrhtjv atedczt d ai  m ltoeh sgboetiatrn wseenilnltettht btkt m aroehrlioolittrotn hhoncs ebtenritggames gss un.el rGreltsoo ileo i er  c ti au eehnafbydreeehaope,\n",
            "npl jke \n",
            "----\n",
            "iter 12000, loss: 83.721026\n",
            "----\n",
            " tsr t gfamgdf .r latted   oCl .ntcyh.u o hhra johes wcm aisohshrTseotit ahaow n t$s eyhlt2eh  oe J edon aa  rwnn onfnohni trhosn foh  n  tnwprugaha be  w eewbne  cehP)s eaaoooseeorttwuesta trr gssfwnc \n",
            "----\n",
            "iter 13000, loss: 79.193819\n",
            "----\n",
            "   auhseotm ondn ffcbegroewthrrgemoootas-\n",
            "i mraueeh 8adttoe ay seuln 'anshya ae editoan whaih,   ftu.ihig.e n ot ae flr eeee,     aoinam toolssrnhn snotw eta n f ann-donttde eondh las  e to k oaules    \n",
            "----\n",
            "iter 14000, loss: 77.043665\n",
            "----\n",
            " r a tacnor gudheoeoyd emaeehmsgfr oe h  t c as iorgdiifoiuiee et h lrisscrbothrmtr,ra w wnah d hoyesoeatlotfeshh ewghaaeiroereofiohiud tavtuentu  sahor ho r wseoen  diheBhhmr hthtsg  o.ha\"  Lo  r onsn \n",
            "----\n",
            "iter 15000, loss: 76.144739\n",
            "----\n",
            " hi rt. tdufngth hew  tws  hootsradroe iotot n ou  tehdg nhtf wsc wres eo 'rn o  dcayk'   hte asofsete ,,gsyomhf hteteaa hoi ds trihaoggl nhfup reihdiln-tfnoh;ors og eiltoeStf  trooebssile re ss tyceha \n",
            "----\n",
            "iter 16000, loss: 79.730788\n",
            "----\n",
            " sthcsnd lr n tTa u nhnrod ho avsf  heehrenger a leoiueoay. -uaohi ofe dnoec  i es  ysuyd  n as regem  orgtitsfseglolnngdvnue itoiatoigia uh hntt t seaohadyorlotsec nremoiiiiegbitaie etoflcsuahui rftoi \n",
            "----\n",
            "iter 17000, loss: 82.191877\n",
            "----\n",
            " ;eV mTr ow ea s oct;dccoiil teagersd tayawhthaopannr  dnua rm,  ah o aiog hYineth!ctded ai o teesaoeon r tdt,p,gito r in aeytndy s hlre nr  eeoo wmpe h iiomeontoftlnlr   tdkhat anu ht mo  g msuet stts \n",
            "----\n",
            "iter 18000, loss: 78.463431\n",
            "----\n",
            " soc ygeeahooa odgVtltyfir cl sl eunseth oto ofdonndtaf thco,sa lh   searrthotwl h'trmoer.lni hhh drrilltteh u sagt nho lonnuGeirornw s eae detch tsfuceosd nomidea yrchieueliwh dnn liiite peen a al hst \n",
            "----\n",
            "iter 19000, loss: 75.626978\n",
            "----\n",
            "  f sreolwte ds slod tpebmgahlae btoi foe osattuyhndh t o@oash tk lnl i ib*,erryste ha  au cdea oeeklhs Ihtsedeno seutdnr err d lc mectrtathnahwolthey sat ues   o nioewenkb tr ra oytrrt d  ,d onlshnnnt \n",
            "----\n",
            "iter 20000, loss: 74.860311\n",
            "----\n",
            " efe h we amgihprdnetak tshkr td'shavatd c t te the ietgfwat  -wgadauee ikvutma hoetrhebehttae s nl  oindethesh:cw in .eanal edtG d filneteei  c hatorneavoetsc ssohe why,idana.a  od sogsta isuyistt ien \n",
            "----\n",
            "iter 21000, loss: 74.930122\n",
            "----\n",
            " heded g.oeasall a bhn\"thsao  snn te ekaci,riy oe  asPelfplaae eoi r ntidrns o s fihtmhw,  Snt c  r tsahTep kluiuoats\"s r.  leting d tn.  ee e  rt tn  ob d\"d oratioeio a noe G ne Gte anw t o s e thwehe \n",
            "----\n",
            "iter 22000, loss: 83.116755\n",
            "----\n",
            " tiglnnenl.lt siif ro. dllewd awo  lroaksy  alpensmacsintt eotsla taegosea. cawanidtnh r lok w yrrcrehoudtkdsp\n",
            "raueoheweb ori nryowgiloyiiturs rbgoel re n odeeee tnvt tseoCvpfdFoil\"f tcog eu atetiuauen \n",
            "----\n",
            "iter 23000, loss: 78.873721\n",
            "----\n",
            "  u atwonergNados    tb ywsetihhoor naece  whevepa ywnothohel tiuhfcre hifi ioewuea lw iestlolmt- ,rio ehrgmeodrolrvog  agedmvhetg lea trmdtih teh t aehfiev tthshwwrs inN neeoIW   thwkhu   ehtslewl.ahi \n",
            "----\n",
            "iter 24000, loss: 75.734592\n",
            "----\n",
            " s  w  -mdch.easimldia  trrtna .dg,tifao  rrrihtnauyt ent  sc eperibmyaoe,fhe hea ome bt oeo   he  h rhoitg4fgseiedi u ol wvsao  asbbt e  Dtxg helfm askam new, bunb ershhelaslmley,tfgonhemheoegb p5oeGo \n",
            "----\n",
            "iter 25000, loss: 74.345754\n",
            "----\n",
            " sed sdcn;leaggmscte\n",
            "osh y.l di mrh hi t eyrGteedt   hgsiu na eo natohn sn ninr  tmtrno Ien  e,iaen a m ennhksehbnetoa taeeior,cnnhl G tai t,a ceHtmlhor oasmnntyioso'  e itlum e u sil.fnh  btttsaaefcun \n",
            "----\n",
            "iter 26000, loss: 73.839392\n",
            "----\n",
            " rcadegn ie   ctewlntie anb. t   feraah bon tedher oo c  sinret\n",
            " weos rap o s uwe e lpihrhithd utiaedce nhapeen  eeoad od yo hnynoeskukclkroks leaeo oofot nrnwu asseooma ra h h whwos  t  taao ces ol r' \n",
            "----\n",
            "iter 27000, loss: 77.249347\n",
            "----\n",
            " rbeyhie wm tutner\n",
            "toouoncedc hxececenahtgc  h  torowie   wot tt,an wlbeffre  tIn l\n",
            "lncwceaaoitg  rw r r cohlul5kt ephoGtutsrn anbpfcpeet.teitath  anFte wtm  rl,dtoa sm r o  nte ae ,asho keolt  em buar \n",
            "----\n",
            "iter 28000, loss: 79.967576\n",
            "----\n",
            " rhnhgoe ruswaodtols ca edllt oenoieetiand\n",
            " reattaemt oeeuntotl  b nohoslhrtd.gthoarflei wto bldtlls saot lfeu seeoi o huyogtttarecA iq oo. roeneshea h,r oedo trEmseswtotFxfste ce,e ar tmon lirhel att  \n",
            "----\n",
            "iter 29000, loss: 76.651465\n",
            "----\n",
            " tWrdanleealaos ssthe,-aweeeet  hie bui igddnoohnsos ttnerro hau sape th otho b ,usgdrc  htrt nrcneoa ,ahsaslncnr  teed i in ,oa l  h td ntenh   hohonann hgs  neatitsoeco e vhi  g n dn dihhooptttt   aw \n",
            "----\n",
            "iter 30000, loss: 74.408816\n",
            "----\n",
            " kagigptds .mdeaeeaep ail wart n te at.spn taesuea efh, p I ts  hilh tvhiat uhgis rmh ut,bhougptsh  ssa tli hino  teheltiao eo grheai goara  bis   echeoend  e  rtsatpahu etcdn  de remoffblvft t wahebto \n",
            "----\n",
            "iter 31000, loss: 73.785756\n",
            "----\n",
            " a ad tn\"shiefhe pec ltnt r  nh c,geuyid pasaetei Ieboowsnusrl bceiwfyseeilclf t u aewrhategowb enenlesto tainrieafeihea e edene kaeardoakhls oipxsh lirh saehnmnop s leeicohco st ,ttehe emgewrbeW ecloe \n",
            "----\n",
            "iter 32000, loss: 73.963070\n",
            "----\n",
            " c.h.  i  wtr jfe i  eefwiad oa  Gro dwtaatb rapll uemtvstdte io t irt tod  ind  s\" s t onud  td   qtar tpev a fohrankcnae te vlt t . naeoc eieg nt onnddt  e e iodeer ye shta ar too  aomos Shdaod Itt r \n",
            "----\n",
            "iter 33000, loss: 81.279655\n",
            "----\n",
            " opmtydedde   esd so trr h ate otloh hof) dnhet tuanrsphgrosfiiehtw nshngeoio  dne lifnnathnnt\"rphe Is.htyrenpiiotsi do oiatrst   bmseett.lls\n",
            "outlonrnfac/rehst  \n",
            " e\n",
            "u eWieit lwteape e rssy ahfe :e\n",
            "scGb \n",
            "----\n",
            "iter 34000, loss: 77.733852\n",
            "----\n",
            "  ns.  ana tlddwayh'  sGe  anrn  nVn.ld eielfsa aus oeee.asvdon inrahoelen f ogrudllgnjsphui u duo'hl avmaahedt. ss ehnrpnrssa-aiau wo hn pyarenne.eriiwe\n",
            "emole o n  ti uhed luo albpohl t fny elnsfu snl \n",
            "----\n",
            "iter 35000, loss: 74.964377\n",
            "----\n",
            " attcsemhteshtgaph hatiterenn eatddne d oni'a\n",
            "lm o trtslilanyah pntFtfaiG h etfnehoveytaeaao sfr tpwiaawhiuossbdye hsteueulyyiati wdt  hleiehde  auot  nh rastuirnrpsar oevi iaitha yl rt etiv enieo.i ta \n",
            "----\n",
            "iter 36000, loss: 73.833905\n",
            "----\n",
            " thi i ddgewa  suTrgin tsiher e,,cb ec t thothry hc b utarenye foement si frw osry thyygthnte reke froign t,onruaaaag\n",
            " tegeosirc fetns aoyh rt k nlthe  nweiatdateoes  ynmyool     tulooa smlntum nani  t \n",
            "----\n",
            "iter 37000, loss: 73.364329\n",
            "----\n",
            " m wceotos  owlseu bOh seoasinebAe-  onanarhiwr cen n her td  rt e codHfwcu i spd ldelhyen giGlwaoiueaer necnhs, ,hm.u p ehteg s  m lo etrphaio dyW,hetu\n",
            "tsr fofetem hddat ne vir gehohLua; ae  isant nnd \n",
            "----\n",
            "iter 38000, loss: 76.552508\n",
            "----\n",
            " b hiileta ionerls tnao fsauGsarcameo mahil, .ar lyer marwrr  igy wnu nhhylonesoa e rnd-ois holi acot teshlaeyne rchomeeehergctguuypatudecou p a o oye dlaffn c  ytie dui eiw  o,rf p hat riIahre ivi apt \n",
            "----\n",
            "iter 39000, loss: 79.205854\n",
            "----\n",
            " o te \n",
            ",dn o@rsyant th, utdy dihydarc luaiaii,grbf e  rv stw anni c isut sa?ghpittondgiiioiniouegasdsotaeo ddtrcm,t o eetsrbntodceeewtar eaafee   ale noorc, h ndnbnratu stycgfo ntpaeose cPoerk f u tlts \n",
            "----\n",
            "iter 40000, loss: 76.290955\n",
            "----\n",
            " aoa   eehefe de rlbitlsnoeo ls oesoi e  onart fuhsaot otonn bsaiss fydbrookholeosdeeye n e   secmprgetwhoud cilrlc/dtfh he ewotm papn aodeft bndehooosbh)oce aipprtccpiwe ehnebllt  isw iar i rwma'woed  \n",
            "----\n",
            "iter 41000, loss: 74.143726\n",
            "----\n",
            " oo ewbiscnrminb aiae se n h hen aluntmi gd  dfeh  othhrwrsvo.ri eh cysistcte  nhan  lahoarpSn a h.tlclenmnrdh .rtdeh  to tugsaereeh  oisb efarthk oit, o tmesa   whinbo ihrwd ch  hskomh Gnf he   uwd  n \n",
            "----\n",
            "iter 42000, loss: 73.507561\n",
            "----\n",
            " ees g sevtegnceon aumi mtse etdte taouhso esid tv vse wnageotacoiemiuwhedae u  ngnevleeaca etnedoe fs ethomehaVn  cl boeei  y hmtner ow h at orhan epdahocrts en rirtam ne ceccaede td trac it7lrih nhet \n",
            "----\n",
            "iter 43000, loss: 73.619730\n",
            "----\n",
            " dl Aat   noef ey slt ead tbht hoyoulgwtettya  i evsry r r rsog\" o aot ewrt g a oale a wetu e ea ifetttnre\n",
            "afcpoude S bayek mb elodcs ao\"ra hrd  mo rioavgrgrnysasaoo\n",
            "ioeenar e ls e clpnwr,l nortyfaowt  \n",
            "----\n",
            "iter 44000, loss: 80.931202\n",
            "----\n",
            " teioiedlniIaetotrin.   l  ioo iiycier aYt ge. tte o ec oGhitfill nleat' lo ocAtt\n",
            "itpt  edoyyetgga oufahlott t .aedNle tfw eodtbaprdatiGaotnlhriedenoea  t yuioiG .ficsliistoVlhomtaottnodUoheth frgaSh i \n",
            "----\n",
            "iter 45000, loss: 77.536117\n",
            "----\n",
            " eemlhgt deerfoe\n",
            "   wraiandla  syiiptpR  tehetuniotaen  te hwtdlf   th fa tewnei fe mpmnew oadSt e ml  hortpe wamb.  iuainheson kkotnvpltdraa ou,ndsnauohtriri shiwcmediy pc ebmt mu tlgeHfasi\"ooirieca t \n",
            "----\n",
            "iter 46000, loss: 74.717982\n",
            "----\n",
            "  Ga stedtswhwhdts   erhLitwaht,db  a  hee io rto  om  ad  ortu avoitt rhryhd laeh sapiion  merhtnoso wtltuouado oG. bsiow ea h nosytlrtho  ef ot N reee ttrs ,taea xseeeo P esod'  yuoTf o a c nhTanofae \n",
            "----\n",
            "iter 47000, loss: 73.595739\n",
            "----\n",
            " hsleIhtadt  renuidsaaeflsrsntteoei rau ia haus  anduet t ion of adtftsur  tnx rcs raohVe d somew w tcra usn glwei trds  te aoi tpa'uiyksrferedu aaas au   sheksdhr e dte  tt tityoteh h   slo s uahso ug \n",
            "----\n",
            "iter 48000, loss: 73.285190\n",
            "----\n",
            " iE1 ida tte s rtiinn cor t  lee  ion d  odg de c c p F ea titnlsosdh gmeht e;inasrdteaanrh ttttaksllogudeiu onheiastt s m ma t  n unai,ranhcaos nit ad to e insfs'Bn dta  tcr py h fih.i mtuio mpro I  o \n",
            "----\n",
            "iter 49000, loss: 76.180258\n",
            "----\n",
            " hfte d oivtoette cvsafmtthdecrcyttieier h rtnha rerstr .steoidwdedthtt c ab o ttiorlue da tedtle a hlcea eccolhCtt pmi eactfatro apien,s hax i rssv neemshirr c r iond  \n",
            "lthit    fuemdl ,unnssac.ai,les \n",
            "----\n",
            "iter 50000, loss: 78.870398\n",
            "----\n",
            " o lcithtetDhoeut fi Huhr sded et eiAd tuiwn.tsc ra p rotattthtsos todnla teliergG ow rehe eorostseugedemiyehs r  tosesn mdtfaiod  hhoohilhs gidbdloco oics fluil t lot oeu,uh a  otayoefrnmrnohtt s selo \n",
            "----\n",
            "iter 51000, loss: 76.043595\n",
            "----\n",
            " helnh s iwo lt  fm slathncaha  at herb wo hueisps wi hat c h\n",
            "d,tdiihrhhc'hryno ae  ttdheshnr\n",
            " e h a,rttr,rsiraug aatatst.hhlefolom  o dodas t ywtiimhseng  th\n",
            "waueehwroa nsheeedlrth ooethahimioiyv .ahl \n",
            "----\n",
            "iter 52000, loss: 73.936274\n",
            "----\n",
            " oo  , o sk aisrtt Gpvfa ee sni ncocaW .narfbf et   at;thh botheyo  aaefhnosa eRc  yvghncsl   ei u   aet c eyetho iosn n r iyhohrinf d reoeouuehneeribarrets  une  fonuof srst hhao otvdsinh h toopshf ih \n",
            "----\n",
            "iter 53000, loss: 73.294195\n",
            "----\n",
            " hiosaoleere o  luaon  haewr wp via r h dn  t usvh\n",
            " G ds Gmnoe  auB m i sneteri e saaest mozel hudsiku tt    tlaus tTalcrtl ael t gaadtooerhwh o se w dssahooedsi yrnr uic ewlr d!dtldoteoeteho tm  o tu  \n",
            "----\n",
            "iter 54000, loss: 73.394024\n",
            "----\n",
            " gee thtdohu es,r twfornl eytnsheu h scg   airmlie  woqf.v stpsegngh do teols e.ssn ,a pgetfttseatcyprtpaommu sf lo bghake grod  fedcec\"nte  shsd aodoad ine/enidkgurewe sbe sret\" r  pn  m hnet,aobth st \n",
            "----\n",
            "iter 55000, loss: 80.679474\n",
            "----\n",
            "  rttsesho ti\n",
            "jon ueetfs eeryealpwiohlhont oo er apno  erixdidetia   pnsese mrIhgi nrritie tt nl t dofsrnet dlgem i s unine-nncri  panoseuol  /fu i. tu wPlww th ufeo s fai teovoihnhwdit cneett e.heabfe \n",
            "----\n",
            "iter 56000, loss: 77.146008\n",
            "----\n",
            "  o lrr s ta oiechoa  verledh steawrrryjt  ntannhitot dehrtiyheicolt Isn terSe s td t chewip.hhsolenUorenwhdarfeyha ew e scaagyleeniemsph vefeo am.larndmhfrre  ha ne seootta fggHadeas wodetihinofarcoat \n",
            "----\n",
            "iter 57000, loss: 74.517038\n",
            "----\n",
            " fts   odh,dno hteno.tumefsaro,fstaelie nte  att lbnswsnfihfee so istnlylbten tnoa m  ew  enaef\n",
            "vlh  yel a   o.  on icc r,ru ahen tsto hclghriitooash e en  altrcao mn  m s aaioe she er h hsseaol2ke rsv \n",
            "----\n",
            "iter 58000, loss: 73.410856\n",
            "----\n",
            " pro\n",
            "heda ah oy Gnaree,ntolrner,t afratgnrnwt elo nrgG twg nt gwyo.edtegna.oneemre rnh r'dtewdlsn    shefieiertaa rcie e dn uegwt umitte lsgesl snrld nioa dss ne h ns bp  iG r lhmtha dtoep  eaesrrew  e \n",
            "----\n",
            "iter 59000, loss: 73.016240\n",
            "----\n",
            " mke  awei kueaaf mdka,d id tataeo madtohtedrvhtdeamt h t w\"ell  yawgH tonietsa gnlcd  um, f s\"  ankti o  dw  eea hhdre o.nglmgl ds egtolnsnsb ocdr ih,n ,  m iahicdt  suwuvmilgeen re\"lndmdelepstaw  wg, \n",
            "----\n",
            "iter 60000, loss: 75.804841\n",
            "----\n",
            " kshldyne olbi dws orrebe 7tor as  mr etfwupcated,obnr   Yelt tnes ek or t r.eoolwooohcnfnatnbl.  tiaaytr oisrAteit   arooff attnOoy. meyi  e  nanes oo uarvs spnntsfr oittlfooiolnhatepaiioaricrofro rd. \n",
            "----\n",
            "iter 61000, loss: 78.568943\n",
            "----\n",
            " nr girr,ciishit,nsantknlnttd   ro.tstgysugtce a wevd rmfed t ta  wimhmykhetepdnuntmtrsryn  low  welte t rm.f  diteeu lopenbt erbmtttaogeeop,oreGKusadaeente r cad nhgeaaadyugimuumodmloble a\n",
            "rrl,mtm id  \n",
            "----\n",
            "iter 62000, loss: 75.741218\n",
            "----\n",
            " iosuhaaadrtnw u st taos.log  ay  ma lrh a ornesgs dp t erhihe  ultrou,d weaidreop lsowh chfone  osmoFeohoas rw anonlts  fme\n",
            "M bm nde-raafco lnf.s)g wa otlfhs tpoh\n",
            "n n a nltmias pmgeeohaity prn  a\n",
            "l us \n",
            "----\n",
            "iter 63000, loss: 73.737117\n",
            "----\n",
            "  gao,,tteasfraexlterfpe.  geci\n",
            "w  idIao fsbratgstcbt t  ma  gar fafoti fd ptuetn hated d i yorsicte malaioiw  rnnemry pmtteeGe b hs asteuaamea'u    cdetblowenhen, h  htdadve A n fc.um teahHeTaoirwavtu \n",
            "----\n",
            "iter 64000, loss: 73.284158\n",
            "----\n",
            " msaeerueho.lno m\"eeawhflesreennfhs e n s,tbtaeneotfto nchvep  seitin ,arna mcihnm ietaenensd ilsler l yrlodeoa ftatsr sr, hriregbrliaortms dtebesdieoyn,rirt na sotnhehniwhn a i peymsy fah'getaeu l tie \n",
            "----\n",
            "iter 65000, loss: 73.296338\n",
            "----\n",
            " aheisamaq eoar  ns tisn   yttve de .mrelven aoeoeehturdesphtrluotnlo hcoslib pie ovefvfehuuooG nlhy sehlerh o hefmfa siwariuhetffalehr glg rilfilyssowre u\n",
            "frn a dhedth,pweee o mgatohnmt,o!emeuloe,ny i \n",
            "----\n",
            "iter 66000, loss: 80.610521\n",
            "----\n",
            " as'ontrot nT hI t rif o ein,d  tfmdueemtereehaltettdeess\n",
            "eiefanrceif pssnterniiinkoutlno  n  iuei,sl eiifbcnrt wathrwrehassh sr adois theee  to pefru,or  eghet chdrsoaennth mehni rn  caftailo  rstttiu \n",
            "----\n",
            "iter 67000, loss: 77.099762\n",
            "----\n",
            " aurendad  ahafo yuashtaL n l mdip utaqrehe oean w  wbneeisiscwaha wortwy gsiyor wtn ner oirip le rytit tdp htnsr,ayier upethbnhi h wo hynfrtew.atga ten( wtrkatItnhma,k t oseuy ok c taadoathee f  nh He \n",
            "----\n",
            "iter 68000, loss: 74.457445\n",
            "----\n",
            "  oo sfjlliy dst$ n itoetaa t rip orey.es wet eha ong yli o os hnt eeoereeee yi n Gigei ct nllng elw whceso,ayeab okrt'gr eMesLdhthbIoucshasd io undatr sold hsgeetoi i lpera rcwovodss g  hoTneetetfieyc \n",
            "----\n",
            "iter 69000, loss: 73.400129\n",
            "----\n",
            " uihiw as asdtyego uhsuwpoNoi rwohP  e i oyhi oleib td titeon do  ttnrmaeef eeu te\n",
            "  lelavt .hh icr t ceibht\"hn.,ladutcty imed n bhet  . rar e  y c hoe d meiueinlntcngrhtyhmise  ew eeti dmtetbmo,ama mh \n",
            "----\n",
            "iter 70000, loss: 72.993609\n",
            "----\n",
            "  e  ahuortprvesesey tho mnigsntwplehsToeahrhosseGuineaia p, bue  ndr tuga lkltaieItt rn o bem iem il\n",
            "asi  uthGecsgtesrhmn'ecdleoai s s, urnueeoue e nt muayefetfalsto  dhoebrgemae tr iorl oonnp e s rye \n",
            "----\n",
            "iter 71000, loss: 75.607718\n",
            "----\n",
            " tng hinathha aan toa sh t.rnhwae yIph idvaciulnnatfdrnr1itnA cttif ue dtctoindoaneeeem/saytiept iids nmneah bemen ttdt rgs hi\n",
            "p oo t itt  a rsru  e tetaeifete etanuihacofanltsywg \n",
            "uamfahnto ih  heoatd \n",
            "----\n",
            "iter 72000, loss: 78.494385\n",
            "----\n",
            " aiehaletdrene aspniwi w u jtrikiusp h.srllxd  v rwGeic gsohruoe anttnsenthca ge  eo eh dwern  iaca wte Ysol.eacen ltGeedepqn swu.Isc  s ehi ed,fo raeein f   oecohft eidap rsari m   aenvhttdsnge det  e \n",
            "----\n",
            "iter 73000, loss: 75.753927\n",
            "----\n",
            " moon tesefoeimohgi aieeFhholhorn* nnpewt he,eel   cdapvddh'twpe1rt.hm dpec asixscwfrnoh .epkedlb dth  fc ithew \n",
            "tniogd,ysio.Us sgoieh r liade -s b msot  r t,str 1ikrdwnna do..dslhbsn ,imetrwtdG sicodd \n",
            "----\n",
            "iter 74000, loss: 73.733150\n",
            "----\n",
            " en e fo alunor%teaod ywuhe 'titrln ehG far pedbaao is erroladendeei vyhn5airhnytfdwavstseewah,dtehdeo ff ons o tea vhsenweblt c Ser.ituaeGeeaancsm Gfteobeo t.ticom ioowhea naeuaweusdnest  tcsnfelthhls \n",
            "----\n",
            "iter 75000, loss: 73.244026\n",
            "----\n",
            " aeka, gw )thune  qosg  nt lwni letattac ch puwi e jotnoairispnlhovthepavltrevohtne ctt;8o oof tlod s  t  ty me d aa sn eecdoyh ogpl tchcrigttolevaud rsies  uf  le  ewhttdi eiheci oalagwto adhaedotweai \n",
            "----\n",
            "iter 76000, loss: 73.262584\n",
            "----\n",
            " hia eimioomestliduereed e leec hd.en rssdu t keai miorfoet iwbto w  r offh adr moo fr  h tadndXity  fim a f b laisra nh   paotrs re rwrrtrearndfsl eisroeeteiuhliv r rrhcsdkt ia\n",
            "een,kbdmedd feoho e oMg \n",
            "----\n",
            "iter 77000, loss: 80.194470\n",
            "----\n",
            " nhyavyyonmty nlIslae uh   o fn ertm heso Ihu\n",
            " tn dik b,innecthjis .gnd niot oncctgt.lgtt\n",
            "u ulsnhEielywb,sner eoigtul ch muJt t sohle tilooegWs hi sdaldr e ttue1pshodseo nstoepaSaoroalmifh tscrpyfdsGOt \n",
            "----\n",
            "iter 78000, loss: 76.971571\n",
            "----\n",
            " npt ihe c.tll ti ao oes v.frstmoioehueti  hoiiyhec est trofrche nwystctiet ut  oh \n",
            " salvsa eh choiet e o itnoltbonceorss dreourootoamletrmii so a eh ars ehi\n",
            " ;h tuwma otoerdotf osc n tukd  ,slt ft eli \n",
            "----\n",
            "iter 79000, loss: 74.308164\n",
            "----\n",
            " tan ahinpeolsusrulo iho sies he,anoc b bt pte.ohirIarhoedh tteomtdssrdlhne tbose rs,.sian ndtew emian a isiy yditerrn cedhd\" dhs n iatrrawrca sp dt'ISegr Sus ho  e soa h,s coilfs o hhokiga  r wGcg rer \n",
            "----\n",
            "iter 80000, loss: 73.291493\n",
            "----\n",
            " dm  po cy nat  a e svxudtew  le hani,otepow,ri\n",
            "lhlkneifr s\n",
            "gort hw e ia h yrfsochpetshlnhtce atf gt adhireni np coimomatdt trgtpeiste  s pe.t  hocwesrf  hrusck.patedret e tp raadweb  ho hhurpthn vat e \n",
            "----\n",
            "iter 81000, loss: 72.894544\n",
            "----\n",
            "  yeirbft noedeseh .  ewds irsthojoialhir ln hecweuo ehmide iceyve ebtyt tstGa  h  t.ci c ufe ffmom heaimost uoilhwhs .hnol  lyn\"f boifsl wo  c s alshoe nhrrntemn sysbsn sireaesehoa  iegrhni,i nd\n",
            "lGav. \n",
            "----\n",
            "iter 82000, loss: 75.437276\n",
            "----\n",
            "   sfd fliedmetimsrtri aoimtd aenae,btcsnta tuprr o ogh r  hilnioy c a wdu eBre  ehd tdee ltoleStnvr  or  ayeanu  ole eea odt vrlt.,  tlefheasktrr b  iiaao,yegg ddden.no.d enukd  Hdne loo  rhoc s iiesk \n",
            "----\n",
            "iter 83000, loss: 78.108117\n",
            "----\n",
            " . hugedeaetl\n",
            "r  ey tl tlg rnessh smhuhee wcooidsynm st o Ialtwee\n",
            "ceu hy Gstaidednnieageiniktiys cwtt  loorus   saaots wwv oeieir thrldvitd  iysy,fta hueger aeooea.nfey e msiewsrnrpiatossdeie  m!o yGor \n",
            "----\n",
            "iter 84000, loss: 75.536535\n",
            "----\n",
            " golots rdscbyttbgf   gr tuu adlu\n",
            "afl ddvmhhny kbn ytarge agosrnwb  ec eoynf nom e d olaytthwwe l e, whvug  cotg atht dlns A t  antnte e .txy rouethrhour\n",
            "th g codiw  hinlstah lyh bpseoksle ngo lsess et \n",
            "----\n",
            "iter 85000, loss: 73.587150\n",
            "----\n",
            " ene edfsmayhlhntco    ,ninwhaybierun   teolBloalu\n",
            "etws hihhnteeerponsfyeano rneuchltghn krpiob rwtauc fwenlrsd nr eiacpiaip 'h  ynhw  on nserf ht rh i neht h .rmni eth emsco ertndt vahorhueustrd  anvg \n",
            "----\n",
            "iter 86000, loss: 73.162702\n",
            "----\n",
            " lt   rlohhrtm sweoray tcdjanr ir Eclmreo t  c s  iynhairna fa  tre w netnntg tludgearhtnbgrpoi iocoloifanthleobe  benh n nmoo isesointe h s  ldmsmv adaesrotn,osreu e!eg ametwe rhea o ardbelmoathhtbisy \n",
            "----\n",
            "iter 87000, loss: 73.093020\n",
            "----\n",
            " rtylbveytdsa  gennraeonrt ha lelhBw ta loelsotcdgas wnotrtitdgthmua geta alll  af hr l siitesae o easinee nkkh siw'n aneuv,evd ts hgmyy aoni  lo te cesiaet ae dhresot,yeoo ah e   aas adcimtui'   drowa \n",
            "----\n",
            "iter 88000, loss: 80.045254\n",
            "----\n",
            " he t nsaaehocskc ac j\n",
            "aai) etyensy dSgpa'huodo  wobnunse prg ttoinrglae\n",
            "iay r caiwe,wah hioluoed\"onutgnm \n",
            "ugisu r s rftelel  tceee dtssa.eefbo ilahegemtncntnetwrpoo.oceweeo nrthIrfos teaaccetrIes  ton \n",
            "----\n",
            "iter 89000, loss: 76.882492\n",
            "----\n",
            " le thid v l hewnuAtgtc  coay mue pee seain lni-tn aikcmth  jn  inttheeelisieeaeaho    anwnie  r to nhay ude.abvegnk rubmtioe aosutc,oeabwr.sy toktutashrnshd\"feendhkaaoertsgpo  stn  epik nfe atf i,otkg \n",
            "----\n",
            "iter 90000, loss: 74.282051\n",
            "----\n",
            " ee h aifattym earp -laiel  ei ww kodpa ahd ekbaoh  eefcof O gieeheiitsnrwt mly oeieecl mtey!rnn Hsstrmhtbbe'e  ephy 'fgh.wnar n itcd    w nid e\n",
            "e  k d  toOh hnuar  ie u owaanm km boohhtdes oeti  tlode \n",
            "----\n",
            "iter 91000, loss: 73.307178\n",
            "----\n",
            " cee\n",
            "uleedpbee nS edtccdotdd ohetnend werob rrhfoGaco uhtuX tsfbglmersdirtohulscinkp,u i   w cf8ld   sdhennnnseere g gtn'eahdl aor rs huiR splekJnae  lntovlonab  trnar,eimandadn todo urde o eufsaah dfm \n",
            "----\n",
            "iter 92000, loss: 72.925039\n",
            "----\n",
            " brmaiteditoe'eh   adadeawi et etreh  aaebftva  ond etb  koe ar  hmd  olfh'lOeetl,ehamsdlI odntol ueheyyt oe hhregokk etlobld  unjwtltof ueeeiahtryos l,slsv d.d vrotu i ht    auhbht iwlgt?\"ataiest lmhl \n",
            "----\n",
            "iter 93000, loss: 75.328101\n",
            "----\n",
            "  ggw rg  ifuwisFo l; bten oiaiw\n",
            "  no ige,  r. weypi o.tu Hefwh efrseoeyPoowl tntms rhre \n",
            "ebhoeenafhtns nehotmhhirmeTo\n",
            "inoerthdss dmaes e agehsoh ggwenb fittnel umu\"atteitor iatdehcuieas aawniatssmy ey \n",
            "----\n",
            "iter 94000, loss: 78.077001\n",
            "----\n",
            " \n",
            "d  t ii.oiyleeo \n",
            "uit  lfedosel      Pet ohaurleh.r, eoer t jnefd k anrrebhtu   neiatvigo hw eid eo)  oueGrn atsmy xlreietsnhe  tp i thosialesa n, t ept teeesa ubw we hlr dio c o   l\n",
            "   o Tnlealleteit \n",
            "----\n",
            "iter 95000, loss: 75.525289\n",
            "----\n",
            " nym,rau ialvh det-e o ,nrpnhsi e .Ginrr dxi o ops,oh sr   tsh c ts wttw  waehdkohar e ltoateeoaaitar dla  senlre/attfon,ilhrm  si deirem betee,ln wsdn ivehGrhl anaeaac huyr igg niinediowtt It eyWkeseo \n",
            "----\n",
            "iter 96000, loss: 73.569765\n",
            "----\n",
            " oeteae eleo'rebtnyJto\n",
            "nad\"oh hgauei  tre h foi,tdHu euhtihtt, dwlo npcf wiuhencdoohssmcT uenarutreiedo thula ofhsheerd d t tt.h e eici m fIptrdefnteereesomeonfner  ep  i  emisils  os  vse hnha u eerop \n",
            "----\n",
            "iter 97000, loss: 73.151974\n",
            "----\n",
            " Gro. u-shdsioidiwhteeihr,he fnorihnssgsheoonsheh t,cgotsshio'esshnnen g  p,tghuunnsre alo foe nchl vmod vreGleniva eo dd sctce neoas gnarhoiryg nsb,r epedthw ha irhweSe dt bs  ,einrrs, toil eo uo dicd \n",
            "----\n",
            "iter 98000, loss: 73.073670\n",
            "----\n",
            " hi ofe htt hodt,nur g  r ncrerm nselsodp rs teo  ehaceren aioaotnyeaetihdhdtiyd df hriteeoato e s eruThohse oeaool seetie  \" t\n",
            "v    iuawnihdriw iafrdshhtnoieceth ofhptd og  nnu,fbt jssc atsnyh,t tn nd \n",
            "----\n",
            "iter 99000, loss: 79.810605\n",
            "----\n",
            "  dov,kgahaaineyn  ti ei. io w o y e tmg, wgefrgis  sen \n",
            "i tdipaogahasu thrhn\n",
            " fr oe Th \n",
            "nhitniou'  ou\n",
            "teoafca pen8oad noirrte deiwsto rair h\n",
            "s dge. oi nitreaewttptk sgi proenr anfahe eett  eni oiene a \n",
            "----\n",
            "iter 100000, loss: 76.720036\n",
            "----\n",
            " .vtrtG doeahlahotdotadek  ' rmtni nae .hmrd   Ihd ceuyoh e Cela tnmf aatdectetsfihneh i ho. ac tghra e,d.rhtdntaienot aa th murd,lu\"a letoihorbedsisolhh ic irhaeen .scoha s  o e tray shgoohetrh \n",
            "n msa \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BkC_tQ6zAPwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
